# Kubernetes Web App
This is a simple Python Flask based Web App that showcases a local kubernetes setup running on Docker by Kind and implements monitoring with Prometheus and Grafana, and Scales up the Web App using HPA when CPU Utilization passes a certain threshold.   

# Setup 

### Requirements
- Docker
- kubectl 1.14 or newer
- Python3.x (with venv)
- Go

### Installation
The `startup.sh` script does all the job. Just run it as the following:
```shell
 bash startup.sh
```

This installation script does the followings:  
- Install kind, and cloud-provider-kind to provide LoadBalancer.  
- Install Kind's suggested Nginx Ingress Controller, and apply proper patches to expose Metrics server for Prometheus.    
- Install Kind's suggested Metrics server to be utilized by HPA.  
- Build the local image for WebApp and load it into Kind to make it accessible to Kubernetes.  
- Install CRDs for cert-manager which is used to Issue SSL/TLS certificates.  
- Install WebApps Helm Chart.  
- Start cloud-provider-kind to provision external IP for Ingress.  
- Wait for the installation to finish, Pick up the load balancer IP that is assigned to Ingress and add and entry to `/etc/hosts` in order to make the app accessible via `https://webapp.local` (Requires root privilege)

Once The installation is finished, you get an output like the following:
```text
Congratulations! Installation Finished.
Web App is accessible at https://webapp.local
API Key: 5eK8fMkQBokTHXvi
Test /calculate endpoint with: curl --insecure -H 'apikey:5eK8fMkQBokTHXvi' https://webapp.local/calculate?param=10
```

### Uninstall
The `cleanup.sh` script does the job. Just run it as the following:
```bash
 bash cleanup.sh
```

This script removes the custom entry from `/etc/hosts`, stops the LoadBalancer, and removes the cluster.  

# Architecture
Using Helm as package manager, helped streamlining the application development as it made it easier to include dependencies such as kube-prometheus-stack, postgresql, and cert-manager.  

As for the application manifests, using helm's templating features made managing variables more efficient.  

The secrets such as DB_PASSWORD, or API_KEY are generated securely upon installation using Helm's random generator and aren't stored anywhere increasing security.  
With utilizing lifecycle hooks, it's assured that operations like helm upgrade or helm rollback won't overwrite these secrets.  

Using Helm's jobs, a job is defined to provision the Database, to set up App specific User and Database and run database migrations.  

Production level measurement had been taken for deployment, with defining tight security contexts, and proper readiness and liveness probes.  

Namespaces are utilized to isolate different parts of the app:  
- web-app: contains the webapp and Postgresql.
- ingress-nginx: contains Nginx Ingress resources.
- monitoring: contains Prometheus/Grafana monitoring stack resources.
- cert-manager: contains resources used by cert-manager  

# Miscellaneous

### Load Testing
Using Locust you can load test the webapp and see HPA in action:  
```shell
cd load-test
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Then add the API Key you got from the startup.sh output and add it to an env variable and start locust:  
```shell
export APIKEY=5eK8fMkQBokTHXvi
locust
```

This will expose a dashboard to http://127.0.0.1:8089 which you can open and perform the stress test.

### Accessing Grafana and Prometheus dashboards
Prometheus dashboard on http://127.0.0.1:9090  
```shell
kubectl port-forward service/web-app-kube-prometheus-st-prometheus 9090:9090 -n monitoring
```

Grafana Dashboard on http://127.0.0.1:3000   
```shell
kubectl port-forward service/web-app-grafana 3000:80 -n monitoring
```

# Troubleshooting
Common Kubernetes troubleshooting methods could be used:  

Get logs for a pod, or a job
```shell
kubectl logs job.batch/web-app-db-provision -n web-app
```


Watch events for a Deployment, or HPA:  
```shell
kubectl events --for hpa/web-app --watch -n web-app
```

Sometimes simply Getting the objects from a namespace and checking the Status column can give you an initial clue:  
```shell
kubectl get pods -n web-app
```

Getting into a pod's shell could also be useful sometimes as you can run commands inside the container:  
```shell
kubectl exec -it pod/web-app-8684b98964-9rgk4 -n web-app -- sh
```

Since resources are generated by helm, looking up their final specification could also be useful sometimes:  
```shell
kubectl get deployment/web-app -o yaml -n web-app
```